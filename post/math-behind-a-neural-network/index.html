<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <meta property="og:site_name" content="Wahab Ahmad">
    <meta property="og:type" content="article">

    
    <meta property="og:image" content="https://w28ahmad.github.io/">
    <meta property="twitter:image" content="https://w28ahmad.github.io/" />
    

    
    <meta name="title" content="Math Behind a Neural Network" />
    <meta property="og:title" content="Math Behind a Neural Network" />
    <meta property="twitter:title" content="Math Behind a Neural Network" />
    

    
    <meta name="description" content="Step by step explination of the math involved in neural networks. Derivation of how commonly used ML equations arise.">
    <meta property="og:description" content="Step by step explination of the math involved in neural networks. Derivation of how commonly used ML equations arise." />
    <meta property="twitter:description" content="Step by step explination of the math involved in neural networks. Derivation of how commonly used ML equations arise." />
    

    
    <meta property="twitter:card" content="summary" />
    
    

    <meta name="keyword"  content="Wahab, Math, Machine Learning">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>Math Behind a Neural Network-Wahab&#39;s Blog</title>

    <link rel="canonical" href="/post/math-behind-a-neural-network/">

    <link rel="stylesheet" href="/css/iDisqus.min.css"/>
	
    
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    
    <link rel="stylesheet" href="/css/zanshang.css">
    
    
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    
    

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css" integrity="sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js" integrity="sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError : false
        });
      });
    </script>


    
    <script src="/js/jquery.min.js"></script>
    
    
    <script src="/js/bootstrap.min.js"></script>
    
    
    <script src="/js/hux-blog.min.js"></script>

    
    

</head>



<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Wahab Ahmad</a>
        </div>

        
        
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                        
                        <li>
                            <a href="/categories/economics">economics</a>
                        </li>
                        
                        <li>
                            <a href="/categories/interview-prep">interview-prep</a>
                        </li>
                        
                        <li>
                            <a href="/categories/machine-learning">machine-learning</a>
                        </li>
                        
                        <li>
                            <a href="/categories/math">math</a>
                        </li>
                        
                        <li>
                            <a href="/categories/programming">programming</a>
                        </li>
                        
                    
                    
		    
                        <li><a href="https://wahabahmad.ca/">ABOUT</a></li>
                    

                    
                </ul>
            </div>
        </div>
        
    </div>
    
</nav>
<script>
    
    
    
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        
            $navbar.className = " ";
            
            setTimeout(function(){
                
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>




<style type="text/css">
    header.intro-header {
        background-image: url('/')
    }
</style>
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/machine-learning" title="Machine Learning">
                            Machine Learning
                        </a>
                        
                        <a class="tag" href="/tags/math" title="Math">
                            Math
                        </a>
                        
                    </div>
                    <h1>Math Behind a Neural Network</h1>
                    <h2 class="subheading"></h2>
                    <span class="meta">
                        Posted by 
                        
                                Wahab Ahmad
                         
                        on 
                        Sunday, September 5, 2021
                        
                        
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>




<article>
    <div class="container">
        <div class="row">

            
            <div class="
                col-lg-11 col-lg-offset-1
                col-md-10 col-md-offset-1
                post-container">

                
                <header>
                    <h2>Contents</h2>
                </header>
                <nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#what-is-a-neural-network">What Is a Neural Network</a>
      <ul>
        <li><a href="#neuron">Neuron</a></li>
        <li><a href="#layer-of-neurons">Layer of Neurons</a></li>
        <li><a href="#neural-network">Neural Network</a></li>
        <li><a href="#the-bias">The Bias</a></li>
      </ul>
    </li>
    <li><a href="#forward-propagation">Forward Propagation</a></li>
    <li><a href="#backward-propagation">Backward Propagation</a></li>
  </ul>
</nav>
                
                <h2 id="overview">Overview</h2>
<p>Machine Learning and AI is all the talk these days in engineering communities. AI in recent years achieved a great mile stone, it can now defeat any individual in any board game ever conceived by humans. Fully autonomous cars are coming closer reality with every passing year. So now is a good time as any to start to learn what is machine learning all about. The post will go through some of the core properties of neural networks such as <strong>Forward Propagation</strong> and <strong>Backward Propagation</strong> with some mathematical rigour.</p>
<p><em>This post will assume you have basic knowledge of Linear Algebra and Calculus.</em></p>
<h2 id="what-is-a-neural-network">What Is a Neural Network</h2>
<p>A neural network is comprised of some number of layers, where each layer is comprised of some number of neurons. You may ask, what is a neuron?</p>
<h3 id="neuron">Neuron</h3>
<p>A neuron is has the structure:</p>
<p>
  <img src="/static/posts/image-20210905194711152.png" alt="Neuron">


$$
\begin{aligned}
n &amp;: \text{number of neurons on the previous layer} \newline
W &amp;: \lbrack w_1, w_2 \dots w_n \rbrack	\text{ weights from previous layer to this layer} \newline
I &amp;: \lbrack i_1, i_2 \dots i_n \rbrack	\text{ inputs from previous layer}\newline
\end{aligned}
$$
$G$ is a function that takes in $W$ and $I$ and determines if the neuron will fire. More precisely, we can define $G$ like so:
$$
\begin{equation}
G(W, I) = \sigma(W \cdot I)
\end{equation}
$$
So, $G$ performs a dot product between W and I, essentially taking a weighted sum of the previous layer: $w_1 \times i_1 + w_2 \times i_2 + \dots + w_n \times i_n$. This sum is then passed into the activation function $\sigma$, and then the activation function lets us know whether or not the neuron fired. A common activation function is the <strong>sigmoid</strong>:
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

  <img src="/static/posts/image-20210905211342520.png" alt="Sigmoid">

</p>
<p>Note that the sigmoid function resembles the step function. If the sigmoid of the weighted sum is above $0.5$ then we can think of the neuron as firing. There are few important properties of the sigmoid function - it is differentiable, which is important for back propagation. It is bounded, it helps us determine which neurons have fired. There are several other types of activation functions each with their own set of benefits and drawbacks. For instance, it has been shown that the sigmoid some times leads to the <strong>Vanashing Gradient Problem</strong> which is detrimental for a neural networks ability to learn.</p>
<h3 id="layer-of-neurons">Layer of Neurons</h3>
<p>It is exactly what it sounds like, a layer of neurons:</p>
<p>
  <img src="/static/posts/image-20210905213610884.png" alt="Layer">

</p>
<h3 id="neural-network">Neural Network</h3>
<p>So finally a neural network is a set of layers:</p>
<p>
  <img src="/static/posts/image-20210905222209442.png" alt="Network">

</p>
<p>In a neural network, we usually have 3 types of layers. Input, hidden and Output layers. The input layers are responsible for supplying the subsequent hidden layer with inputs. The hidden layer is where all the magic happens, each neuron performs its function as defined above with the supplied inputs from the previous layer. The output layer gives the output of the neural network.</p>
<h3 id="the-bias">The Bias</h3>
<p>You may have noted something weird about the image of the neural network above. Why aren&rsquo;t the green neurons accepting any inputs? Well, they are special type of neurons called <strong>Bias</strong> neurons. Their speciality is that they always output a one.</p>
<p>
  <img src="/static/posts/image-20210905222643706.png" alt="Bias">

</p>
<h4 id="why-do-we-need-a-bias-neuron">Why do we need a bias neuron?</h4>
<p>To understand why we need a bias neuron, we can run a thought experiment. Assume we build a neural network with 1 input neuron and 1 output neuron without any bias neurons and we adjust the weight from input to output such that for an input of $0$ we want an output of $0$. Assume we use a sigmoid activation function. The result of the output neuron will be so:
$$
\begin{aligned}
G(W, I) &amp;= \sigma(w_1 \times 0) \newline
&amp;= 0.5
\end{aligned}
$$
Note that no matter what we choose for $w_1$ we will always get an output of $0.5$ and so we can never teach the neural network to output a $0$. Now if we include a bias neuron, the result of the output neuron will be so:
$$
\begin{aligned}
G(W, I) &amp;= \sigma(w_1 \times 0 + w_2 \times 1) \newline
\end{aligned}
$$
Now note that choose a sufficiently large negative value for $w_2$ will result in an output of close to $0$. Hence, the bias makes the neural network more powerful at learning.</p>
<h2 id="forward-propagation">Forward Propagation</h2>
<p>To understand forward propagation we need mathematical notation for the entire network rather than just the neuron. Therefore we define the following variables:
$$
W^{(l)} = \begin{bmatrix}
w_{0,0}^{(l)} &amp; \dots &amp; w_{0,n}^{(l)} \newline
\vdots &amp; \ddots &amp; \vdots \newline
w_{m+1,0}^{(l)} &amp; \dots &amp; w_{m+1,n}^{(l)} \newline
\end{bmatrix}
$$</p>
<p>*<em>Note that $^{(l)}$ not an exponent</em></p>
<p>We start by define $W^l \in \real^{m+1 \times n} $ where $w_{i. j}^l$ is the weight from the $i$-th neuron on layer $l-1$ to the $j$-th neuron on layer $l$.
$$
I^{(1)} = \begin{bmatrix}
i_0 \newline
\vdots \newline
i_m
\end{bmatrix}
$$
Next, we define the input vector at layer $1$, where $m$ input neurons are chosen for simplicity. In reality every layer could have an arbitrary number of neurons. Finally we define forward propagation from input layer to the first hidden layer as such:
$$
\begin{equation}
I^{(2)} = \sigma(I^{(1)T} \times W^{(2)})\newline
\end{equation}
$$</p>
<p>$$
I^{(2)}=\sigma\Bigg(
\begin{bmatrix}
i_0 \newline
\vdots \newline
i_m \newline
1
\end{bmatrix} ^ T
\times
\begin{bmatrix}
w_{0,0}^{(1)} &amp; \dots &amp; w_{0,n}^{(1)} \newline
\vdots &amp; \ddots &amp; \vdots \newline
w_{m+1,0}^{(1)} &amp; \dots &amp; w_{m+1,n}^{(1)} \newline
\end{bmatrix}
\Bigg)
$$</p>
<p>*<em>Note that $^{T}$ is the transpose operation</em></p>
<p>So, the inputs to layer to can be computed as above. Each subsiquent layer can be computed in a similar way.</p>
<h2 id="backward-propagation">Backward Propagation</h2>
<p>The purpose behind back-prop is to adjust the weights at each layer $W^{(l)}$ such that the output of the neural network $\bar{O}$  moves closer to the actual output $O$. To do so, we first must define the concept of error, a measure for how far $\bar{O}$ is from $O$.
$$
\begin{equation}
E = \frac{1}{2}\sum_{i=1}^{n}(o_i-\bar{o_i})^2
\end{equation}
$$</p>
<p>$$
\text{Where $n$ is the number output of neurons}
$$</p>
<p>We want to minimize $w_{i, j}^l$ such that the error is minimized. To do so, lets consider minimizing a simpler function:
$$
f(x) = x^2 \newline
\text{suppose we are currently at $x_o=-3$}
$$
We can adjust the $x$ and move it in the direction of decreasing slope like so:
$$
x_o = x_o - f&rsquo;(x)\Bigr\rvert_{x=x_o}\newline~
\newline
\text{Iteration 1:}\newline
x_o = -3 - f&rsquo;(x)\Bigr\rvert_{x=-3}\newline
x_o = 3 \newline~
\newline
\text{Iteration 2:}\newline
x_o = 3 - f&rsquo;(x)\Bigr\rvert_{x=3}\newline
x_o = -3
$$
Hmm, looks like we aren&rsquo;t making any progress:</p>
<img src="/static/posts/image-20210907152013782.png" alt="stuck" style="zoom:50%;" />
<p>To fix this issue, we must add another a step size / learning rate $\alpha$ to ensure that each subsequent step is smaller than the previous. Suppose $\alpha = \frac{1}{3}$:
$$
\begin{equation}
x_o = x_o - \alpha f&rsquo;(x)|_{x=x_o}
\end{equation}
$$</p>
<p>$$
\text{Iteration 1:}\newline
x_o = -3 - \frac{1}{3}f&rsquo;(x)|_{x=-3}\newline
x_o = -1
$$</p>
<p>$$
\text{Iteration 2:}\newline
x_o = -1 - \frac{1}{3}f&rsquo;(x)|_{x=-1}\newline
x_o = -1/3\newline
\vdots
$$</p>
<p>Now, we can see that $x_o$  is converging towards the minimum. This type of iterative optimization is referred to as <strong>Gradient Descent</strong>. The final tool we need to understand back-prop is the chain rule. Chain rule is a rule that defines derivatives of composition of functions.</p>
<p>$$
f(x) = y(g(x))\newline
\frac{df}{dx} = \frac{dy}{dg}\frac{dg}{dx}
$$
To understand why the chain rule is important we need view the output $\bar{O}$ as a compositions of outputs from previous layers.</p>
<p>$$
\begin{equation}
\bar{O} = \sigma(\sigma(\sigma(&hellip; \times W^{(n-2)}) \times W^{(n-1)}) \times W^{(n)})\newline
\end{equation}
$$</p>
<p><em>Where $n$ is the total number of layers in the network</em></p>
<p>In order to update each weight, everytime we go back one layer we will need to use the chain rule to find the gradient that will minimize the error $E$ with respect to the weight $w_{i,j}^l$ . Once we have the gradient we need to update the weights according to equation (4).
$$
\begin{equation}
w_{i, j}^{(l)} = w_{i, j}^{(l)} - \alpha \frac{\partial E}{\partial w_{i, j}^{(l)}}
\end{equation}
$$</p>


                

                <hr>
                <ul class="pager">
                    
                    
                    <li class="next">
                        <a href="https://w28ahmad.github.io/post/supply-and-demand/" data-toggle="tooltip" data-placement="top" title="Supply and Demand">Next
                            Post &rarr;</a>
                    </li>
                    
                </ul>

                
<div id="disqus-comment"></div>



            </div>
            
            <div class="
                col-lg-11 col-lg-offset-1
                col-md-10 col-md-offset-1
                sidebar-container">

                
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <a href="/tags/programming" title="programming">
                            programming
                        </a>
                        
                        
                    </div>
                </section>
                

                
                
            </div>
        </div>
    </div>
</article>




<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                   
                   <li>
                       <a href='' rel="alternate" type="application/rss+xml" title="Wahab Ahmad" >
                           <span class="fa-stack fa-lg">
                               <i class="fa fa-circle fa-stack-2x"></i>
                               <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                           </span>
                       </a>
                   </li>
                   
                    
                    <li>
                        <a href="mailto:w28ahmad@uwaterloo.ca"
                        rel="alternate" type="application/rss+xml" title="mailto:w28ahmad@uwaterloo.ca" >
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                    
                    
                    

                    

		    
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/w28ahmad" title="GitHub">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                    
                    
                    <li>
                        <a target="_blank" href="https://www.linkedin.com/in/wahab-ahmad/" title="LinkedIn">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
		    
                    
                    
                    
                    
                    
            
            
            
                </ul>
		<p class="copyright text-muted">
                    Copyright &copy; Wahab Ahmad 2023
                    <br>
                    <a href="https://themes.gohugo.io/hugo-theme-cleanwhite">CleanWhite Hugo Theme</a> by <a href="https://zhaohuabing.com">Huabing</a> |
                    <iframe title="Zhao Huabing GitHub"
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>




<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>






<script>
    
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>


<script>
    async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>






</body>
</html>
